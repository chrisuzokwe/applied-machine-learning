# -*- coding: utf-8 -*-
"""Assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aYMxBZ8rOuieAT8xQkWCwM7GulKvACC9
"""

from sklearn.datasets import load_iris
iris = load_iris()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

cd "/content/gdrive/MyDrive/1- Academics/CS 383/Assignment 2"

spamdata = pd.read_csv('spambase.data', header=None)
train = spamdata.sample(frac=(2/3), random_state=0)
test = spamdata.drop(train.index)

trainY = train[57]
trainX = train.drop(columns=[57])

mean = trainX.mean()
std = trainX.std()
trainX = (trainX-mean)/std

testY = test[57]
testX = test.drop(columns=[57])
testX = (testX-mean)/std
#testX.insert(0, 'Bias', 1)

"""# Logistic Regression"""

def sigmoid(x, theta):

  val = -(x @ theta)
  return 1/(1+np.exp(val))

def cost(y, x, thetas):

  activations = y*-np.log(sigmoid(x, thetas)) + (1-y)*-np.log(1 - sigmoid(x, thetas))
  cost = sum(activations)
  return cost

X = iris.data[:, :2]
y = (iris.target != 0) * 1
X[:,0] = (X[:,0] - X[:, 0].mean())/X[:, 0].std()
X[:,1] = (X[:,1] - X[:, 1].mean())/X[:, 1].std()
thetas = np.random.uniform(low=-1, high=1, size=2)
lr = 0.01

bias = np.ones(len(X)).reshape(150, 1)
X_bias = np.concatenate((bias, X), axis=1)

thetas = np.random.uniform(low=-1, high=1, size=3)

iter = 10000
counter = 0
stoppingthresh = 2E-23
old_loss = 1E9
lr = 0.01

while counter < iter:

  counter = counter+1
  loss = cost(y, X_bias, thetas)
  if abs(loss-old_loss) < stoppingthresh:
    break

  thetas = thetas + lr/2* X_bias.T @ (y - sigmoid(X_bias, thetas))
  old_loss = loss

c = -thetas[0]/thetas[2]
m = -thetas[1]/thetas[2]
xsample = np.asarray(list(range(-2, 2)))
ysample = m*xsample + c

thetas

from sklearn.linear_model import LogisticRegression

lgr = LogisticRegression(penalty='none', solver='lbfgs', max_iter=10000)
lgr.fit(X, y)

b = lgr.intercept_
coef = lgr.coef_[0]
c = -b/coef[1]
m = -coef[0]/coef[1]

xsampleval = np.asarray(list(range(-2, 2)))
ysampleval = m*xsampleval + c[0]

plt.scatter(X[:,0], X[:,1])
plt.plot(xsample, ysample, 'r', label='scratch')
plt.plot(xsampleval, ysampleval, 'b', label='sklearn')
plt.title('Logistic Regression')
plt.xlabel('x1')
plt.ylabel('x2')

"""# Logistic Regression Classifier"""

X = trainX.to_numpy()
Y = trainY.to_numpy()
X_test = testX.to_numpy()
Y_test = testY.to_numpy()

bias = np.ones(len(X)).reshape(len(X), 1)
X_bias = np.concatenate((bias, X), axis=1)

thetas = np.random.uniform(low=-1, high=1, size=58)
lr = 0.01

iter = 1500
counter = 0
stoppingthresh = 2E-23
old_loss = 1E9
lr = 0.01

while counter < iter:

  counter = counter+1
  loss = cost(Y, X_bias, thetas)
  if abs(loss-old_loss) < stoppingthresh:
    break

  thetas = thetas + lr/57* X_bias.T @ (Y - sigmoid(X_bias, thetas))
  old_loss = loss

y_score = sigmoid(X_bias, thetas)
y_class = np.zeros(len(y_score))
pos_mask = np.where(y_score > 0.5)
y_class[pos_mask] = 1

def accuracy(y, y_pred):

  total = 0
  for i in range(len(y)):
    if y[i] == y_pred[i]:
      total = total + 1

  return total/len(y)

def precision(y, y_pred):
  tp = 0
  fp = 0

  for i in range(len(y)):
    if y_pred[i] == 1:

      if y[i] == 1:
        tp = tp+1

      else:
        fp = fp+1

  return tp/(tp+fp)

def recall(y, y_pred):
  tp = 0
  fn = 0

  for i in range(len(y)):
    if y_pred[i] == 1:

      if y[i] == 1:
        tp = tp+1

    else:
      if y[i] == 1:
        fn = fn+1

  return tp/(tp+fn)

def f1(precision, recall):
  return (2*precision*recall)/(precision+recall)

acc = accuracy(Y, y_class)
p = precision(Y, y_class)
r = recall(Y, y_class)
f = f1(p, r)

print(acc, p, r, f)

"""# Naive Bayes Classifier"""

def gauss(x, mu, sig):
  exponent = np.exp((-1/2)* ((x-mu)/sig)**2)
  prob = 1/(sig*np.sqrt(2*np.pi)) * exponent
  return prob

X = trainX.to_numpy()
Y = trainY.to_numpy()
X_test = testX.to_numpy()
Y_test = testY.to_numpy()

oneidx = np.where(Y == 1)[0]
zeroidx = np.where(Y == 0)[0]
p1 = len(oneidx)/len(Y)
p0 = 1 - p1

X_class_0 = X[zeroidx].copy()
X_class_1 = X[oneidx].copy()

zero_mean = X_class_0.mean(axis=0).reshape(57, 1)
zero_std = X_class_0.std(axis=0).reshape(57, 1)

one_mean = X_class_1.mean(axis=0).reshape(57, 1)
one_std = X_class_1.std(axis=0).reshape(57, 1)

test_prob_0 = gauss(X_test, zero_mean.T, zero_std.T)
test_prob_1 = gauss(X_test, one_mean.T, one_std.T)

f_test_prob_0 = test_prob_0.prod(axis=1)*p0
f_test_prob_1 = test_prob_1.prod(axis=1)*p1

y_assign = []

for i in range(len(f_test_prob_0)):
  if f_test_prob_0[i] > f_test_prob_1[i]:
    y_assign.append(0)
    
  else:
    y_assign.append(1)

y_pred = np.asarray(y_assign)

acc = accuracy(y, y_pred)
p = precision(y, y_pred)
r = recall(y, y_pred)
f = f1(p, r)

print(acc, p, r, f)

"""# Decision Tree Classifier"""

X = trainX.to_numpy()
Y = trainY.to_numpy()
X_test = testX.to_numpy()
Y_test = testY.to_numpy()

oneidx = np.where(Y == 1)[0]
zeroidx = np.where(Y == 0)[0]
p1 = len(oneidx)/len(Y)
p0 = 1 - p1

X_class_0 = X[zeroidx].copy()
X_class_1 = X[oneidx].copy()

entropy = -p0*np.log2(p0) + -p1*np.log2(p1)

zero_mean = X_class_0.mean(axis=0).reshape(57, 1)
zero_std = X_class_0.std(axis=0).reshape(57, 1)

one_mean = X_class_1.mean(axis=0).reshape(57, 1)
one_std = X_class_1.std(axis=0).reshape(57, 1)

prob_0 = gauss(X, zero_mean.T, zero_std.T)
prob_1 = gauss(X, one_mean.T, one_std.T)

prob_0.dtype

eps = np.finfo(np.float64).eps

mask_0 = np.where(prob_0 < eps)
mask_1 = np.where(prob_1 < eps)

prob_0[mask_0] = eps
prob_1[mask_1] = eps

entropy_all = -prob_0*np.log2(prob_0) + -prob_1*np.log2(prob_1)
IG = entropy - entropy_all.sum(axis=0)/3067

IG.argmax()

entropy_all[31]

prob_0_test = gauss(X_test, zero_mean.T, zero_std.T)
prob_1_test = gauss(X_test, one_mean.T, one_std.T)

prob_0_test[0][31] < prob_1_test[1][31]

Y_test[31]

for i in range(len(prob_0_test)):
  print((prob_0_test[i][31] < prob_1_test[i][31]),Y_test[i])