# -*- coding: utf-8 -*-
"""Assignment1_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Urgm1JagLfsigdSqK4PSFxEHx3mvdXq-

# CS 383 Assignment 1: Regression
"""

import numpy as np
import matplotlib.pyplot as plt

"""#Part 1 - Theory

#### 1a. Computing the coefficients for linear regression with Least Squares Estimate -
"""

X = np.array([[1, -2], 
              [1, -5], 
              [1, -3],
              [1, 0],
              [1, -8],
              [1, -2],
              [1, 1],
              [1, 5],
              [1, -1],
              [1, 6]])

Y = np.array([1, -4, 1, 3, 11, 5, 0, -1, -3, 1])

thetas = np.linalg.inv(X.T @ X) @ X.T @ Y.T
thetas

"""#### 1b. Confirming intercept and coefficients with sklearn"""

from sklearn.linear_model import LinearRegression

reg = LinearRegression().fit(X, Y)
reg.coef_

reg.intercept_

"""#### 2b. 2D plot of x1 vs. J @ x2 = 0, 1, 2"""

def J_calc(x1, x2):
  J = []
  for val in x1:
    J.append(val^2 + x2^2 + 2*val*x2 - 4*val - 4*x2 + 4)

  return J

x1 = list(range(-10, 10))
Jval_0 = J_calc(x1, 0)
Jval_1 = J_calc(x1, 1)
Jval_2 = J_calc(x1, 2)

plt.plot(x1,Jval_0, label='x2 = 0')
plt.plot(x1,Jval_1, label='x2 = 1')
plt.plot(x1,Jval_2, label='x2 = 2')

plt.xlabel("X1")
plt.ylabel("J")
plt.title("x1 vs. J for varying x2")
plt.legend()

"""J is minimized at x1 = 7 and x2 = 0

# Part 2 - Closed Form Linear Regression
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

cd "/content/gdrive/MyDrive/1- Academics/CS 383/Assignment 1"

df = pd.read_csv("x06Simple.csv")

trainX = df.sample(frac=0.68, random_state=0)
testX = df.drop(trainX.index)

trainY = trainX["Length of Fish"]
trainX = trainX.drop(columns=["Index", "Length of Fish"])

mean = trainX.mean()
std = trainX.std()
trainX = (trainX-mean)/std
trainX.insert(0, 'Bias', 1)

testY = testX["Length of Fish"]
testX = testX.drop(columns=["Index", "Length of Fish"])
testX = (testX-mean)/std
testX.insert(0, 'Bias', 1)

X = trainX.to_numpy()
Y = trainY.to_numpy()

thetas = np.linalg.inv(X.T @ X) @ X.T @ Y.T
thetas

testX_np = testX.to_numpy()
testY_pred = testX_np @ thetas

testY_predPD = thetas[0] + thetas[1]*testX["Age"] + thetas[2]*testX["Temp of Water"]

err = np.sqrt(((testY - testY_pred)**2).sum()/len(testY))

err

"""# Part 3 - Locally-Weighted Linear Regression"""

trainX = df.sample(frac=0.68, random_state=0)
testX = df.drop(trainX.index)

trainY = trainX["Length of Fish"]
trainX = trainX.drop(columns=["Index", "Length of Fish"])

mean = trainX.mean()
std = trainX.std()
trainX = (trainX-mean)/std
trainX.insert(0, 'Bias', 1)

testY = testX["Length of Fish"]
testX = testX.drop(columns=["Index", "Length of Fish"])
testX = (testX-mean)/std
testX.insert(0, 'Bias', 1)

X = trainX.to_numpy()
Y = trainY.to_numpy()

thetas = np.linalg.inv(X.T @ X) @ X.T @ Y.T
thetas

def L1(a, b):

  d = 0
  for i in range(len(a)):
    d = d + abs(a[i]-b[i])
  return d

thetas = []

for i in testX.to_numpy():
  d = []
  for j in X:
    d.append(np.exp(-1*L1(i, j)))

  W = np.diag(d)
  thetas.append(np.linalg.inv(X.T @ W @ X) @ X.T @ W @ Y.T)

local_Y_pred = []
X_test = testX.to_numpy()

for i in range(len(thetas)):
  local_Y_pred.append(X_test[i][0]*thetas[i][0] + X_test[i][1]*thetas[i][1] + X_test[i][2]*thetas[i][2])

err = np.sqrt(((testY - local_Y_pred)**2).sum()/len(testY))
err

"""# Part 4 - Gradient Descent"""

trainX = df.sample(frac=0.68, random_state=0)
testX = df.drop(trainX.index)

trainY = trainX["Length of Fish"]
trainX = trainX.drop(columns=["Index", "Length of Fish"])

mean = trainX.mean()
std = trainX.std()
trainX = (trainX-mean)/std
trainX.insert(0, 'Bias', 1)

testY = testX["Length of Fish"]
testX = testX.drop(columns=["Index", "Length of Fish"])
testX = (testX-mean)/std
testX.insert(0, 'Bias', 1)

X = trainX.to_numpy()
Y = trainY.to_numpy()
X_test = testX.to_numpy()
Y_test = testY.to_numpy()

thetas = np.random.uniform(low=-1, high=1, size=3)
lr = 0.01

rmseTrainA = []
rmseTestA = []

iter = 1000
counter = 0
stoppingthresh = 2E-23
rmseOldTrain = 0

while counter < iter:
  counter = counter+1
  grad = 2*X.T @ ((X @ thetas)-Y)
  thetas = thetas - lr*grad

  Y_train_pred = X @ thetas
  rmseTrain = np.sqrt(((Y - Y_train_pred)**2).sum()/len(Y))

  Y_test_pred = X_test @ thetas
  rmseTest = np.sqrt(((Y_test - Y_test_pred)**2).sum()/len(Y_test))

  rmseTrainA.append(rmseTrain)
  rmseTestA.append(rmseTest)

  # print(rmseTrain, rmseTest)
  # print("percent change:", abs(rmseTrain-rmseOldTrain)/rmseOldTrain*100)
  if abs(rmseTrain-rmseOldTrain)/rmseOldTrain*100 <= stoppingthresh:
    break

  rmseOldTrain = rmseTrain

thetas

iter = list(range(0, len(rmseTrainA)))

plt.plot(iter, rmseTrainA, label="Training RMSE")
plt.plot(iter, rmseTestA, label="Testing RMSE")

plt.xlabel("Iterations")
plt.ylabel("RMSE")
plt.title("RMSE For test and training sets during gradient descent")
plt.legend()